# ğŸ“˜ GCP Data API â€“ Student Profile & Taxonomy Service

**FastAPI + BigQuery service powering the E-Portfolio (CV) Generation System**

---

## ğŸš€ Overview

This service provides the **data access layer** for the CV Generation pipeline.
It exposes a clean HTTP API to retrieve:

* **Student full profiles** (nested query, 1 BigQuery call)
* **Role taxonomy** (roles + required skills)
* **Job Description (JD) taxonomy**
* **Template metadata** used by the CV generator

All data lives in **Google BigQuery**.
The API is deployed on **Google Cloud Run** (fully serverless, autoscaling).

---

## ğŸ—ï¸ Updated Architecture Highlights

### ğŸ”¹ **Single nested BigQuery query for `/full-profile`**

Replaced 10 sequential BigQuery queries with **one array-aggregated query**:

* `student`
* `education`
* `experience`
* `skills`
* `awards`
* `extracurriculars`
* `publications`
* `training`
* `references`
* `additional_info`

This reduces latency from **~80 seconds â†’ ~2â€“3 seconds**.

### ğŸ”¹ **Async endpoints ready for Orchestrator parallelization**

The Data API is fully compatible with:

* `httpx.AsyncClient`
* `asyncio.gather`
* Cloud-Run-to-Cloud-Run calls

---

## ğŸ§± Project Structure

```
eport_data_api/
â”œâ”€â”€ api.py                         # FastAPI entrypoint (async)
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py                # Reads env vars (project, dataset, region)
â”œâ”€â”€ database/
â”‚   â””â”€â”€ bigquery_client.py         # BigQuery client wrapper (sync)
â”œâ”€â”€ functions/
â”‚   â””â”€â”€ orchestrator/
â”‚       â””â”€â”€ eport_data_gathering_orchestrator.py  # Hydration logic
â”œâ”€â”€ repositories/
â”‚   â”œâ”€â”€ student_repo.py            # Nested full-profile BigQuery query
â”‚   â”œâ”€â”€ role_repo.py               # Role/JD/template queries
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ parameters/
â”‚   â”œâ”€â”€ schemas/                   # Optional schema definitions
â”‚   â””â”€â”€ keys/                      # (ignored)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ example_input_tables/      
â”‚   â””â”€â”€ load_csv_files2_bigquery.py# Bulk CSV â†’ BigQuery loader
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Features

### âœ” **Fast Student Full-Profile Aggregation**

`/v1/students/{user_id}/full-profile`

Powered by a single BigQuery query with `ARRAY(SELECT AS STRUCT ...)`
Handles missing tables gracefully (returns empty lists).

### âœ” **Role Taxonomy**

`/v1/roles/{role_id}/taxonomy`

Includes:

* Role metadata
* Required skills (ordered)

### âœ” **JD Taxonomy**

`/v1/jds/{jd_id}/taxonomy`

Includes:

* Job metadata
* Required skills
* Responsibilities

### âœ” **Template Metadata**

`/v1/templates/{template_id}`

Used by the CV Generation Orchestrator.

---

## ğŸ› ï¸ Running Locally

### 1. Install dependencies

```bash
uv sync
```

or:

```bash
pip install -r requirements.txt
```

### 2. Authenticate via GCP service account

```bash
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/key.json"
```

### 3. Run FastAPI

```bash
uvicorn api:app --reload --port 8001
```

Open docs:

* [http://localhost:8001/docs](http://localhost:8001/docs)
* [http://localhost:8001/redoc](http://localhost:8001/redoc)

---

## ğŸ”½ Loading Sample Data Into BigQuery

Use included loader:

```bash
python tests/example_input_tables/load_csv_files2_bigquery.py \
    --project_id YOUR_PROJECT \
    --dataset_id gold_layer \
    --location asia-southeast1
```

Supports schema autodetect + BOM-safe CSV headers.

---

## ğŸŒ API Endpoints

### Students

| Method | Endpoint                              | Description           |
| ------ | ------------------------------------- | --------------------- |
| GET    | `/v1/students/{user_id}/core`         | Raw student row       |
| GET    | `/v1/students/{user_id}/full-profile` | Hydrated full profile |

### Roles

| Method | Endpoint                       | Description     |
| ------ | ------------------------------ | --------------- |
| GET    | `/v1/roles/{role_id}/core`     | Raw role record |
| GET    | `/v1/roles/{role_id}/taxonomy` | Role + skills   |

### JDs

| Method | Endpoint                   | Description         |
| ------ | -------------------------- | ------------------- |
| GET    | `/v1/jds/{jd_id}/core`     | Raw JD record       |
| GET    | `/v1/jds/{jd_id}/taxonomy` | JD + skills + tasks |

### Templates

| Method | Endpoint                      | Description   |
| ------ | ----------------------------- | ------------- |
| GET    | `/v1/templates/{template_id}` | Template info |

---

## â˜ï¸ Deploying to Cloud Run

### Build with Cloud Build

```bash
gcloud builds submit --tag gcr.io/PROJECT_ID/eport-data-api
```

### Deploy

```bash
gcloud run deploy eport-data-api \
  --image gcr.io/PROJECT_ID/eport-data-api \
  --region asia-southeast1 \
  --platform managed \
  --allow-unauthenticated \
  --set-env-vars GCP_PROJECT_ID=...,BQ_DATASET=gold_layer,BQ_LOCATION=asia-southeast1 \
  --service-account=YOUR_SERVICE_ACCOUNT
```

---

## ğŸ”’ Security Notes

* Never commit keys in `parameters/keys/`
* Use **GCP Secret Manager** for production secrets
* Cloud Run uses service account identity â†’ no JSON keys needed in production
* Enable GitHub Secret Scanning

---

## ğŸ§© Future Enhancements

* Add `async BigQuery client` for fully async repo layer
* Add Redis/Memorystore caching
* Publish OpenAPI schema to Artifact Registry
* Add Cloud Build CI/CD pipeline
* Add response Pydantic models

---
